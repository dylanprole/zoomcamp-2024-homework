{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0f3e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f913615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/25 02:06:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Begin spark session on port 4040\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49159ea6",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "Install Spark and PySpark\n",
    "\n",
    "Install Spark\n",
    "Run PySpark\n",
    "Create a local spark session\n",
    "Execute spark.version.\n",
    "What's the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "483e0d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\r\n",
      "      ____              __\r\n",
      "     / __/__  ___ _____/ /__\r\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\r\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.2\r\n",
      "      /_/\r\n",
      "                        \r\n",
      "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.2\r\n",
      "Branch HEAD\r\n",
      "Compiled by user liangchi on 2023-02-10T19:57:40Z\r\n",
      "Revision 5103e00c4ce5fcc4264ca9c4df12295d42557af6\r\n",
      "Url https://github.com/apache/spark\r\n",
      "Type --help for more information.\r\n"
     ]
    }
   ],
   "source": [
    "!spark-shell --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56305c5b",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "FHV October 2019\n",
    "\n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8072aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom schema\n",
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PUlocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOlocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9718b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into spark cluster\n",
    "df = spark.read \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('../data/raw/fhv/2019/10/fhv_tripdata_2019_10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef1f0f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(6).write.parquet('../data/pq/fhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f3707cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 39M\r\n",
      "-rw-r--r-- 1 proled proled    0 Feb 25 02:11 _SUCCESS\r\n",
      "-rw-r--r-- 1 proled proled 6.4M Feb 25 02:11 part-00000-b52fd8c6-133e-4a15-a222-28721d46fa62-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 proled proled 6.4M Feb 25 02:11 part-00001-b52fd8c6-133e-4a15-a222-28721d46fa62-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 proled proled 6.4M Feb 25 02:11 part-00002-b52fd8c6-133e-4a15-a222-28721d46fa62-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 proled proled 6.4M Feb 25 02:11 part-00003-b52fd8c6-133e-4a15-a222-28721d46fa62-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 proled proled 6.4M Feb 25 02:11 part-00004-b52fd8c6-133e-4a15-a222-28721d46fa62-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 proled proled 6.4M Feb 25 02:11 part-00005-b52fd8c6-133e-4a15-a222-28721d46fa62-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ../data/pq/fhv/2019/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293de6b6",
   "metadata": {},
   "source": [
    "## Question 3:\n",
    "Count records\n",
    "\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c5adb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rename = df \\\n",
    "    .withColumnRenamed('dropOff_datetime', 'dropoff_datetime') \\\n",
    "    .withColumnRenamed('PUlocationID', 'pickup_id') \\\n",
    "    .withColumnRenamed('DOlocationID', 'dropoff_id') \\\n",
    "    .withColumnRenamed('SR_Flag', 'sr_flag') \\\n",
    "    .withColumnRenamed('Affiliated_base_number', 'affil_base_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec8af03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rename.createOrReplaceTempView('fhv_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ea13be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark.sql(\"\"\"\n",
    "\n",
    "SELECT\n",
    "    COUNT(*)\n",
    "FROM\n",
    "    fhv_data\n",
    "WHERE\n",
    "    CAST(pickup_datetime AS DATE) = '2019-10-15'\n",
    ";\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cc8f832f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 65:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2760c5f1",
   "metadata": {},
   "source": [
    "## Question 4:\n",
    "Longest trip for each day\n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "64a88732",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark.sql(\"\"\"\n",
    "\n",
    "SELECT\n",
    "    UNIX_TIMESTAMP(pickup_datetime) AS pickup_unix,\n",
    "    UNIX_TIMESTAMP(dropoff_datetime) AS dropoff_unix,\n",
    "    UNIX_TIMESTAMP(dropoff_datetime) - UNIX_TIMESTAMP(pickup_datetime) AS trip_length_seconds,\n",
    "    (UNIX_TIMESTAMP(dropoff_datetime) - UNIX_TIMESTAMP(pickup_datetime)) / (60*60) AS trip_length_hours\n",
    "FROM\n",
    "    fhv_data\n",
    "ORDER BY\n",
    "    (dropoff_datetime - pickup_datetime) DESC\n",
    "LIMIT\n",
    "    10\n",
    ";\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6e9c24b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 46:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|trip_length_hours|\n",
      "+-----------------+\n",
      "|         631152.5|\n",
      "+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.select('trip_length_hours').show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70b53c4",
   "metadata": {},
   "source": [
    "## Question 6:\n",
    "Least frequent pickup location zone\n",
    "\n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8697cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom schema\n",
    "schema_zones = types.StructType([\n",
    "    types.StructField('LocationID', types.IntegerType(), True),\n",
    "    types.StructField('Borough', types.StringType(), True),\n",
    "    types.StructField('Zone', types.StringType(), True),\n",
    "    types.StructField('service_zone', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ccc1e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into spark cluster\n",
    "df_zones = spark.read \\\n",
    "    .schema(schema_zones) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('../data/raw/zones/taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a70c55f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones_renamed = df_zones \\\n",
    "    .withColumnRenamed('LocationID', 'location_id') \\\n",
    "    .withColumnRenamed('Borough', 'borough') \\\n",
    "    .withColumnRenamed('Zone', 'zone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fde9098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones_renamed.createOrReplaceTempView('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5749ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_3 = spark.sql(\"\"\"\n",
    "\n",
    "SELECT\n",
    "    zones.zone,\n",
    "    COUNT(*)\n",
    "FROM\n",
    "    fhv_data\n",
    "INNER JOIN\n",
    "    zones\n",
    "ON\n",
    "    fhv_data.pickup_id = zones.location_id\n",
    "GROUP BY\n",
    "    zones.zone\n",
    "ORDER BY\n",
    "    COUNT(*)\n",
    "LIMIT\n",
    "    1\n",
    ";\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d5b1499e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 62:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|       zone|count(1)|\n",
      "+-----------+--------+\n",
      "|Jamaica Bay|       1|\n",
      "+-----------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result_3.show(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
